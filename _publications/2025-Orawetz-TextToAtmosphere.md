---
title: From Text to Immersion&colon; A Modular Software Pipeline to Generate Audiovisual Environments from Text Prompts
author: Orawetz, J., Kammer, D., Freitag, G.
link: https://doi.org/10.1145/3743049.3748579
year: 2025
venue: MuC '25&colon; Proceedings of the 2025 Mensch und Computer 2025
type: conference
tags: multimodal
bibtex: >-
    @inproceedings{10.1145/3743049.3748579,
    author = {Orawetz, Jimmy and Kammer, Dietrich and Freitag, Georg},
    title = {From Text to Immersion: A Modular Software Pipeline to Generate Audiovisual Environments from Text Prompts},
    year = {2025},
    isbn = {9798400715822},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3743049.3748579},
    doi = {10.1145/3743049.3748579},
    abstract = {Creating high-fidelity audiovisual content for immersive environments typically requires time-consuming manual design. To accelerate the design of digital environments for immersive media, this paper presents a modular generative-AI pipeline that transforms a single text prompt into synchronized panoramic visuals and ambient soundscapes for CAVE-style projection setups, VR, and other non-standard aspect ratios. The visual module leverages open-source diffusion models with iterative outpainting, seam correction, and high-resolution upscaling, while the audio branch uses multimodal Large Language Models for captions and sound synthesis to generate context-aware one-minute sound stems. All processing runs locally on consumer-grade hardware. The technical evaluation across five- and three-sided projection scenarios demonstrates an average generation time of less than a minute. These results confirm the effectiveness of the pipeline for rapid prototyping and deployment of high-resolution panoramic imagery and ambient soundscapes from a single user-provided prompt.},
    booktitle = {Proceedings of the Mensch Und Computer 2025},
    pages = {610â€“616},
    numpages = {7},
    keywords = {multimodal content generation, interactive media, immersive environments},
    location = {
    },
    series = {MuC '25}
    }
---
Creating high-fidelity audiovisual content for immersive environments typically requires time-consuming manual design. To accelerate the design of digital environments for immersive media, this paper presents a modular generative-AI pipeline that transforms a single text prompt into synchronized panoramic visuals and ambient soundscapes for CAVE-style projection setups, VR, and other non-standard aspect ratios. The visual module leverages open-source diffusion models with iterative outpainting, seam correction, and high-resolution upscaling, while the audio branch uses multimodal Large Language Models for captions and sound synthesis to generate context-aware one-minute sound stems. All processing runs locally on consumer-grade hardware. The technical evaluation across five- and three-sided projection scenarios demonstrates an average generation time of less than a minute. These results confirm the effectiveness of the pipeline for rapid prototyping and deployment of high-resolution panoramic imagery and ambient soundscapes from a single user-provided prompt.
